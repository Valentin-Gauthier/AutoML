{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b08e001a-3775-42f5-ad1a-76cb0e33628b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[init] Warning: the provided folder cant be clean : [Errno 39] Directory not empty: 'automl_logs'\n",
      "[load dataset] Loading Dense dataset: data_A\n",
      "[fit] multilabel_classification task detected.\n",
      "\n",
      "    Dataset Target Analysis (34190 samples)\n",
      "   Type: Multi-label (3 labels)\n",
      " - Average labels per sample: 2.28\n",
      " -> Label 0  : 29207  (85.43%)\n",
      " -> Label 2  : 26013  (76.08%)\n",
      " -> Label 1  : 22847  (66.82%)\n",
      "\n",
      "\n",
      "\n",
      "[fit] Candidate models loaded for task 'multilabel_classification':\n",
      "   1. Random Forest Classifier Multi-label\n",
      "   2. K-Neighbors Classifier Multi-label\n",
      "   3. Gradient Boosting Classifier Multi-label\n",
      "   4. Hist Gradient Boosting Multi-label\n",
      "   5. MLP Multi-label\n",
      "   6. Ridge Classifier\n",
      "   7. Linear SVC Multi-label\n",
      "   8. Extra Trees Classifier Multi-label\n",
      "\n",
      " [filter models] Dynamic Model Filtering ---\n",
      " Dataset Info: 27352 rows, 215 cols, Sparse=False\n",
      " Target Info : 3 labels (Multi-label)\n",
      " -> Models selected: 8 / 8\n",
      "\n",
      "[CLUSTER] Optimizing: Random Forest Classifier Multi-label...\n",
      "[CLUSTER] Success (369.3s). Best params: {'estimator__n_estimators': 53, 'estimator__max_depth': None, 'estimator__min_samples_leaf': 1, 'estimator__n_jobs': 1, 'estimator__class_weight': 'balanced'}\n",
      "[fit] Retraining final model on full data...\n",
      "[fit] Final training done in 5.4s.\n",
      "[CLUSTER] Optimizing: K-Neighbors Classifier Multi-label...\n",
      "[CLUSTER] Success (495.8s). Best params: {'estimator__n_neighbors': 15, 'estimator__weights': 'distance', 'estimator__n_jobs': 1}\n",
      "[fit] Retraining final model on full data...\n",
      "[fit] Final training done in 2.9s.\n",
      "[CLUSTER] Optimizing: Gradient Boosting Classifier Multi-label...\n",
      "[CLUSTER] Success (1548.8s). Best params: {'estimator__n_estimators': 177, 'estimator__learning_rate': 0.10039410681030984, 'estimator__max_depth': 4, 'estimator__subsample': 0.9901941044036306, 'estimator__validation_fraction': 0.1, 'estimator__n_iter_no_change': 10}\n",
      "[fit] Retraining final model on full data...\n",
      "[fit] Final training done in 45.3s.\n",
      "[CLUSTER] Optimizing: Hist Gradient Boosting Multi-label...\n",
      "[CLUSTER] Success (642.5s). Best params: {'estimator__learning_rate': 0.05949959063638726, 'estimator__max_iter': 167, 'estimator__max_depth': 10, 'estimator__l2_regularization': 0.19772386427958322, 'estimator__early_stopping': True, 'estimator__n_iter_no_change': 10}\n",
      "[fit] Retraining final model on full data...\n",
      "[fit] Final training done in 8.9s.\n",
      "[CLUSTER] Optimizing: MLP Multi-label...\n",
      "[CLUSTER] Success (253.0s). Best params: {'hidden_layer_sizes': [100], 'activation': 'relu', 'alpha': 0.0005686683128366133, 'learning_rate_init': 0.00117409154647898, 'max_iter': 500, 'early_stopping': True, 'n_iter_no_change': 10}\n",
      "[fit] Retraining final model on full data...\n",
      "[fit] Final training done in 1.9s.\n",
      "[CLUSTER] Optimizing: Ridge Classifier...\n",
      "[CLUSTER] Success (54.5s). Best params: {'estimator__alpha': 1.4036292151039422, 'estimator__class_weight': 'balanced'}\n",
      "[fit] Retraining final model on full data...\n",
      "[fit] Final training done in 1.8s.\n",
      "[CLUSTER] Optimizing: Linear SVC Multi-label...\n",
      "[CLUSTER] Success (78.7s). Best params: {'estimator__C': 0.03389535909217156, 'estimator__max_iter': 2000}\n",
      "[fit] Retraining final model on full data...\n",
      "[fit] Final training done in 1.7s.\n",
      "[CLUSTER] Optimizing: Extra Trees Classifier Multi-label...\n",
      "[CLUSTER] Success (657.0s). Best params: {'estimator__n_estimators': 199, 'estimator__max_depth': 30, 'estimator__n_jobs': 1}\n",
      "[fit] Retraining final model on full data...\n",
      "[fit] Final training done in 15.1s.\n",
      "\n",
      "[eval] --- Detailed Results on Test Set (6838 samples) ---\n",
      "\n",
      " > Model: Random Forest Classifier Multi-label\n",
      "   - F1 (Samples): 0.8965 (Quality per instance)\n",
      "   - F1 (Macro)  : 0.9079 (Quality per label)\n",
      "\n",
      "[Classification Report]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.87      1.00      0.93      5828\n",
      "           1       0.89      0.88      0.88      4560\n",
      "           2       0.88      0.95      0.91      5233\n",
      "\n",
      "   micro avg       0.88      0.95      0.91     15621\n",
      "   macro avg       0.88      0.94      0.91     15621\n",
      "weighted avg       0.88      0.95      0.91     15621\n",
      " samples avg       0.88      0.95      0.90     15621\n",
      "\n",
      "\n",
      " > Model: K-Neighbors Classifier Multi-label\n",
      "   - F1 (Samples): 0.8724 (Quality per instance)\n",
      "   - F1 (Macro)  : 0.8844 (Quality per label)\n",
      "\n",
      "[Classification Report]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.85      1.00      0.92      5828\n",
      "           1       0.82      0.87      0.84      4560\n",
      "           2       0.86      0.92      0.89      5233\n",
      "\n",
      "   micro avg       0.85      0.93      0.89     15621\n",
      "   macro avg       0.85      0.93      0.88     15621\n",
      "weighted avg       0.85      0.93      0.89     15621\n",
      " samples avg       0.85      0.94      0.87     15621\n",
      "\n",
      "\n",
      " > Model: Gradient Boosting Classifier Multi-label\n",
      "   - F1 (Samples): 0.9000 (Quality per instance)\n",
      "   - F1 (Macro)  : 0.9114 (Quality per label)\n",
      "\n",
      "[Classification Report]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      1.00      0.93      5828\n",
      "           1       0.90      0.88      0.89      4560\n",
      "           2       0.89      0.94      0.91      5233\n",
      "\n",
      "   micro avg       0.89      0.94      0.91     15621\n",
      "   macro avg       0.89      0.94      0.91     15621\n",
      "weighted avg       0.89      0.94      0.91     15621\n",
      " samples avg       0.89      0.95      0.90     15621\n",
      "\n",
      "\n",
      " > Model: Hist Gradient Boosting Multi-label\n",
      "   - F1 (Samples): 0.8996 (Quality per instance)\n",
      "   - F1 (Macro)  : 0.9111 (Quality per label)\n",
      "\n",
      "[Classification Report]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.87      1.00      0.93      5828\n",
      "           1       0.90      0.87      0.89      4560\n",
      "           2       0.89      0.94      0.92      5233\n",
      "\n",
      "   micro avg       0.89      0.94      0.91     15621\n",
      "   macro avg       0.89      0.94      0.91     15621\n",
      "weighted avg       0.89      0.94      0.91     15621\n",
      " samples avg       0.89      0.94      0.90     15621\n",
      "\n",
      "\n",
      " > Model: MLP Multi-label\n",
      "   - F1 (Samples): 0.8936 (Quality per instance)\n",
      "   - F1 (Macro)  : 0.9057 (Quality per label)\n",
      "\n",
      "[Classification Report]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      0.99      0.93      5828\n",
      "           1       0.89      0.87      0.88      4560\n",
      "           2       0.88      0.93      0.90      5233\n",
      "\n",
      "   micro avg       0.88      0.94      0.91     15621\n",
      "   macro avg       0.88      0.93      0.91     15621\n",
      "weighted avg       0.88      0.94      0.91     15621\n",
      " samples avg       0.88      0.94      0.89     15621\n",
      "\n",
      "\n",
      " > Model: Ridge Classifier\n",
      "   - F1 (Samples): 0.8084 (Quality per instance)\n",
      "   - F1 (Macro)  : 0.8324 (Quality per label)\n",
      "\n",
      "[Classification Report]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.91      0.74      0.82      5828\n",
      "           1       0.98      0.74      0.84      4560\n",
      "           2       0.94      0.75      0.83      5233\n",
      "\n",
      "   micro avg       0.94      0.74      0.83     15621\n",
      "   macro avg       0.94      0.74      0.83     15621\n",
      "weighted avg       0.94      0.74      0.83     15621\n",
      " samples avg       0.93      0.76      0.81     15621\n",
      "\n",
      "\n",
      " > Model: Linear SVC Multi-label\n",
      "   - F1 (Samples): 0.8935 (Quality per instance)\n",
      "   - F1 (Macro)  : 0.9048 (Quality per label)\n",
      "\n",
      "[Classification Report]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.87      1.00      0.93      5828\n",
      "           1       0.91      0.85      0.88      4560\n",
      "           2       0.87      0.94      0.90      5233\n",
      "\n",
      "   micro avg       0.88      0.94      0.91     15621\n",
      "   macro avg       0.88      0.93      0.90     15621\n",
      "weighted avg       0.88      0.94      0.91     15621\n",
      " samples avg       0.89      0.94      0.89     15621\n",
      "\n",
      "\n",
      " > Model: Extra Trees Classifier Multi-label\n",
      "   - F1 (Samples): 0.8930 (Quality per instance)\n",
      "   - F1 (Macro)  : 0.9045 (Quality per label)\n",
      "\n",
      "[Classification Report]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.87      1.00      0.93      5828\n",
      "           1       0.89      0.87      0.88      4560\n",
      "           2       0.87      0.94      0.90      5233\n",
      "\n",
      "   micro avg       0.88      0.94      0.91     15621\n",
      "   macro avg       0.88      0.94      0.90     15621\n",
      "weighted avg       0.88      0.94      0.91     15621\n",
      " samples avg       0.88      0.94      0.89     15621\n",
      "\n",
      "\n",
      "==================================================\n",
      " BEST MODEL : Gradient Boosting Classifier Multi-label\n",
      " Score      : 0.9000\n",
      " Params     : {'estimator__n_estimators': 177, 'estimator__learning_rate': 0.10039410681030984, 'estimator__max_depth': 4, 'estimator__subsample': 0.9901941044036306, 'estimator__validation_fraction': 0.1, 'estimator__n_iter_no_change': 10}\n",
      "==================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import importlib\n",
    "from src.nevergrad import automl\n",
    "importlib.reload(automl)\n",
    "from src.nevergrad.automl import AutoML\n",
    "\n",
    "automl = AutoML(\n",
    "    budget=40,\n",
    "    feature_selection_threshold=800,\n",
    "    timeout_min=10,\n",
    "    num_workers=10,\n",
    "    mem_gb=8,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "data_dest = \"/info/corpus/ChallengeMachineLearning/data_A\"\n",
    "automl.fit(data_dest)\n",
    "automl.eval()\n",
    "\n",
    "# WARNING : j'ai l'impression que lorsqu'il y a du monde sur le cluster sumbitit se fais bloquer : Error running slurm prolog: 255."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dad59a16-5370-4f31-9890-1c0961a2932f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[predict] Loading data from /info/corpus/ChallengeMachineLearning/data_A...\n",
      "[load dataset] Loading Dense dataset: data_A\n",
      "[predict] Predicting using GradientBoostingClassifier (wrapped in MultiOutputClassifier)...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[1., 1., 1.],\n",
       "       [1., 1., 0.],\n",
       "       [1., 1., 1.],\n",
       "       ...,\n",
       "       [1., 1., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [1., 1., 1.]])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "automl.predict(data_dest)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (autoML_env)",
   "language": "python",
   "name": "automl_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
