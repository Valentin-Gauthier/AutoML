# ==========================================================
# Grilles d'hyperparamètres OPTIMISÉES - Régression
# ==========================================================

Ridge:
  # alpha: Test sur une échelle logarithmique. 
  # Petit (0.01) = proche de la régression linéaire standard. 
  # Grand (100) = forte contrainte (bon si beaucoup de bruit).
  alpha: [0.01, 0.1, 1.0, 10.0, 100.0]

K-Neighbors Regressor:
  # n_neighbors: On teste plus large pour capter des tendances globales (20) ou locales (3).
  n_neighbors: [3, 5, 9, 15, 20]
  weights: ['uniform', 'distance']
  # p: 1 = Distance de Manhattan (souvent meilleure en haute dimension), 2 = Euclidienne.
  p: [1, 2]

SVR:
  # C: Compromis marge/erreur. Échelle logarithmique indispensable ici.
  C: [0.1, 1, 10, 100, 1000]
  # gamma: Définit la portée d'influence d'un point. 
  # Crucial pour le noyau RBF. Si 'scale' ne marche pas, 0.1 ou 0.01 peuvent débloquer.
  gamma: ['scale', 0.1, 0.01, 0.001]
  kernel: ['rbf', 'linear']

Random Forest Regressor:
  # n_estimators: Plus il y en a, plus le modèle est stable (mais lent).
  n_estimators: [100, 300, 500]
  # max_depth: Limiter la profondeur évite d'apprendre le bruit par cœur.
  max_depth: [null, 10, 20, 30]
  # min_samples_leaf: Le paramètre le plus efficace contre le surapprentissage.
  # Empêche de créer une feuille pour une seule donnée isolée.
  min_samples_leaf: [1, 2, 4]
  # max_features: Nombre de variables testées à chaque embranchement.
  max_features: ['sqrt', 'log2', null]

Gradient Boosting Regressor:
  # Stratégie : Learning rate faible + beaucoup d'arbres = meilleure performance généralement.
  n_estimators: [100, 500, 1000]
  learning_rate: [0.01, 0.05, 0.1]
  # max_depth: Doit rester faible pour le boosting (contrairement aux Random Forests).
  max_depth: [3, 5, 7]
  # subsample: Si < 1.0, on fait du "Stochastic" Gradient Boosting (réduit la variance).
  subsample: [0.8, 1.0]

# ===================================================================
# Grilles d'hyperparamètres OPTIMISÉES - Classification
# ===================================================================

Logistic Regression:
  # C: Inverse de la régularisation (100 = peu de régularisation).
  C: [0.01, 0.1, 1, 10, 100]
  # solver: 'liblinear' est robuste pour les petits datasets et gère bien L1 et L2.
  solver: ['liblinear']
  # penalty: l1 (Lasso, peut annuler des variables), l2 (Ridge, garde tout).
  penalty: ['l2', 'l1']

K-Neighbors Classifier:
  n_neighbors: [3, 5, 9, 15, 20]
  weights: ['uniform', 'distance']
  p: [1, 2]

SVC:
  # probability: True est requis si vous voulez utiliser predict_proba plus tard (attention c'est lent).
  # Note: C et Gamma sont les clés du succès ici.
  C: [0.1, 1, 10, 100, 1000]
  gamma: ['scale', 0.1, 0.01, 0.001]
  kernel: ['rbf', 'linear']

Random Forest Classifier:
  n_estimators: [100, 300, 500]
  max_depth: [null, 10, 20, 30]
  min_samples_leaf: [1, 2, 4]
  max_features: ['sqrt', 'log2']

Gradient Boosting Classifier:
  n_estimators: [100, 500, 1000]
  learning_rate: [0.01, 0.05, 0.1]
  max_depth: [3, 5, 7]
  subsample: [0.8, 1.0]

# ===================================================================
# Grilles d'hyperparamètres OPTIMISÉES - Multi-label
# ===================================================================

RF Multi-label:
  estimator__n_estimators: [100, 300]
  estimator__min_samples_leaf: [1, 2, 4]
  estimator__max_depth: [null, 20]

KN Multi-label:
  estimator__n_neighbors: [3, 5, 9, 15]
  estimator__weights: ['uniform', 'distance']

GB Multi-label:
  estimator__n_estimators: [100, 300]
  estimator__learning_rate: [0.05, 0.1]
  estimator__max_depth: [3, 5]