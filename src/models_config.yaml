# ==========================================================
# Grilles d'hyperparamètres pour les modèles de Régression
# ==========================================================

Ridge:
  # alpha: Force de la régularisation (L2). Plus alpha est élevé, plus la régularisation est forte.
  alpha: [0.1, 1.0, 10.0]

K-Neighbors Regressor:
  # n_neighbors: Nombre de voisins 'k' à considérer.
  n_neighbors: [5] # Réduit pour le test
  # weights: 'uniform' (tous les voisins ont le même poids), 'distance' (les voisins proches ont plus d'impact).
  weights: ['distance']
  # IMPORTANT : Force un seul coeur pour éviter le conflit avec GridSearchCV
  n_jobs: [1]

SVR:
  # C: Paramètre de régularisation. Inverse de la force de régularisation (plus C est grand, moins on régularise).
  C: [1, 10]
  # kernel: Type de noyau pour gérer la non-linéarité. 'rbf' est le plus courant.
  kernel: ['rbf']
  # Sécurité pour éviter que le SVM ne tourne à l'infini sur les données sparse
  max_iter: [1000]

Random Forest Regressor:
  # n_estimators: Nombre d'arbres dans la forêt.
  n_estimators: [10, 50] # Valeurs réduites pour tester le pipeline (remettre 100+ plus tard)
  # max_depth: Profondeur maximale de chaque arbre. (J'ai retiré 'null' car trop dangereux sur sparse matrix)
  max_depth: [5, 10]
  # IMPORTANT : Pas de parallélisme interne
  n_jobs: [1]

Gradient Boosting Regressor:
  # n_estimators: Nombre d'étapes de boosting (nombre d'arbres).
  n_estimators: [10, 50]
  # learning_rate: Taux d'apprentissage (réduit la contribution de chaque arbre).
  learning_rate: [0.1]
  # max_depth: Profondeur de chaque arbre. Généralement faible pour le boosting.
  max_depth: [3]

# ===================================================================
# Grilles d'hyperparamètres pour les modèles de Classification
# (Binaire et Multi-classe)
# ===================================================================

Logistic Regression:
  # C: Inverse de la force de régularisation.
  C: [0.1, 1, 10]
  # solver: Algorithme à utiliser pour l'optimisation. 'lbfgs' gère mieux le multiclasse multinomial que liblinear.
  solver: ['lbfgs']
  max_iter: [1000]

K-Neighbors Classifier:
  # n_neighbors: Nombre de voisins 'k' à considérer.
  n_neighbors: [5]
  # weights: 'uniform' (vote à la majorité), 'distance' (les voisins proches ont plus de poids).
  weights: ['distance', 'uniform']
  n_jobs: [1]

SVC:
  # C: Paramètre de régularisation.
  C: [1, 10]
  # kernel: Type de noyau.
  kernel: ['rbf']
  # Sécurité anti-boucle infinie
  max_iter: [1000]

Random Forest Classifier:
  # n_estimators: Nombre d'arbres dans la forêt.
  n_estimators: [10, 50]
  # max_depth: Profondeur maximale de chaque arbre. (Pas de 'null' ici pour l'instant)
  max_depth: [5, 10]
  n_jobs: [1]

Gradient Boosting Classifier:
  # n_estimators: Nombre d'étapes de boosting.
  n_estimators: [10, 50]
  # learning_rate: Taux d'apprentissage.
  learning_rate: [0.1]
  # max_depth: Profondeur de chaque arbre.
  max_depth: [3]

# ===================================================================
# Grilles d'hyperparamètres pour les modèles Multi-label
# ===================================================================

Random Forest Classifier Multi-label:
  # estimator__n_estimators: Nombre d'arbres (passé au RandomForest *dans* le MultiOutput).
  estimator__n_estimators: [10, 20] # Très bas car multiplié par le nombre de labels !
  # estimator__max_depth: Profondeur maximale.
  estimator__max_depth: [5, 10]
  estimator__n_jobs: [1]

K-Neighbors Classifier Multi-label:
  # estimator__n_neighbors: Nombre de voisins.
  estimator__n_neighbors: [5]
  estimator__n_jobs: [1]

Gradient Boosting Classifier Multi-label:
  # estimator__n_estimators: Nombre d'arbres.
  estimator__n_estimators: [10, 20]
  # estimator__max_depth: Profondeur maximale.
  estimator__max_depth: [3]