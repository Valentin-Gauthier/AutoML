# ==========================================================
# Grilles d'hyperparamètres pour les modèles de Régression
# ==========================================================

Ridge:
  # alpha: Force de la régularisation (L2). Plus alpha est élevé, plus la régularisation est forte.
  alpha: [0.01, 0.1, 1.0, 10.0, 100.0]

K-Neighbors Regressor:
  # n_neighbors: Nombre de voisins 'k' à considérer.
  n_neighbors: [3, 5, 9, 15] # Réduit pour le test
  # weights: 'uniform' (tous les voisins ont le même poids), 'distance' (les voisins proches ont plus d'impact).
  weights: ['distance', 'uniform']
  # IMPORTANT : Force un seul coeur pour éviter le conflit avec GridSearchCV
  n_jobs: [1]

SVR:
  # C: Paramètre de régularisation. Inverse de la force de régularisation (plus C est grand, moins on régularise).
  C: [0.1, 1, 10, 100]
  # kernel: Type de noyau pour gérer la non-linéarité. 'rbf' est le plus courant.
  kernel: ['rbf']
  # gamma définit l'influence d'un seul point. 'scale' est le défaut, 'auto' parfois mieux.
  gamma: ['scale', 'auto']
  # Sécurité pour éviter que le SVM ne tourne à l'infini sur les données sparse
  max_iter: [5000, 10000]

Random Forest Regressor:
  # n_estimators: Nombre d'arbres dans la forêt.
  n_estimators: [100, 200] # Valeurs réduites pour tester le pipeline (remettre 100+ plus tard)
  # max_depth: Profondeur maximale de chaque arbre. (J'ai retiré 'null' car trop dangereux sur sparse matrix)
  max_depth: [null, 20]
  min_samples_split: [2, 5] # Optionnel : aide à régulariser si max_depth=None
  # IMPORTANT : Pas de parallélisme interne
  n_jobs: [1]

Gradient Boosting Regressor:
  # n_estimators: Nombre d'étapes de boosting (nombre d'arbres).
  n_estimators: [100, 200]
  # learning_rate: Taux d'apprentissage (réduit la contribution de chaque arbre).
  learning_rate: [0.05, 0.1, 0.2]
  # max_depth: Profondeur de chaque arbre. Généralement faible pour le boosting.
  max_depth: [3, 5]

# Ce model ser pour la regression et la classification (binaire/multiclasse)
Hist Gradient Boosting:
  learning_rate: [0.05, 0.1]
  max_iter: [100, 200]
  max_depth: [null, 10]
  min_samples_leaf: [20]

MLP Regressor:
  regressor__hidden_layer_sizes: [[50], [100], [50, 50]]
  regressor__activation: ['relu', 'tanh']
  regressor__alpha: [0.0001, 0.001]

MLP Classifier:
  hidden_layer_sizes: [[50], [100], [50, 50]]
  activation: ['relu', 'tanh']
  alpha: [0.0001, 0.001]
  max_iter: [5000]

Bernoulli Naive Bayes:
  alpha: [0.1, 0.5, 1.0] # Lissage
  binarize: [0.0, 0.5] # Seuil (utile si données non binaires à la base)
# ===================================================================
# Grilles d'hyperparamètres pour les modèles de Classification
# (Binaire et Multi-classe)
# ===================================================================

Logistic Regression:
  # C: Inverse de la force de régularisation.
  C: [0.01, 0.1, 1, 10, 100]
  # solver: Algorithme à utiliser pour l'optimisation. 'lbfgs' gère mieux le multiclasse multinomial que liblinear.
  solver: ['lbfgs']
  max_iter: [2000]

K-Neighbors Classifier:
  # n_neighbors: Nombre de voisins 'k' à considérer.
  n_neighbors: [3, 5, 9, 15]
  # weights: 'uniform' (vote à la majorité), 'distance' (les voisins proches ont plus de poids).
  weights: ['distance', 'uniform']
  n_jobs: [1]

SVC:
  # C: Paramètre de régularisation.
  C: [0.1, 1, 10, 100]
  # kernel: Type de noyau.
  kernel: ['rbf']
  # gamma définit l'influence d'un seul point. 'scale' est le défaut, 'auto' parfois mieux.
  gamma: ['scale', 'auto']
  # Sécurité anti-boucle infinie
  max_iter: [5000, 10000]

Random Forest Classifier:
  # n_estimators: Nombre d'arbres dans la forêt.
  n_estimators: [100, 200]
  # max_depth: Profondeur maximale de chaque arbre. (Pas de 'null' ici pour l'instant)
  max_depth: [null, 20]
  # Optionnel mais utile pour éviter le sur-apprentissage si depth=None
  min_samples_split: [2, 5]
  n_jobs: [1]

Gradient Boosting Classifier:
  # n_estimators: Nombre d'étapes de boosting.
  n_estimators: [100, 200]
  # learning_rate: Taux d'apprentissage.
  learning_rate: [0.05, 0.1, 0.2]
  # max_depth: Profondeur de chaque arbre.
  max_depth: [3, 5]

Gaussian Naive Bayes:
  # Var_smoothing permet de gérer les distributions un peu irrégulières
  # 1e-9 est la valeur par défaut
  var_smoothing: [0.000000001, 0.00000001, 0.0000001, 0.000001]

# ===================================================================
# Grilles d'hyperparamètres pour les modèles Multi-label
# ===================================================================

Random Forest Classifier Multi-label:
  # estimator__n_estimators: Nombre d'arbres (passé au RandomForest *dans* le MultiOutput).
  estimator__n_estimators: [50, 100] # Très bas car multiplié par le nombre de labels !
  # estimator__max_depth: Profondeur maximale.
  estimator__max_depth: [10, 20]
  estimator__n_jobs: [1]

K-Neighbors Classifier Multi-label:
  # estimator__n_neighbors: Nombre de voisins.
  estimator__n_neighbors: [5, 9]
  estimator__n_jobs: [1]

Gradient Boosting Classifier Multi-label:
  # estimator__n_estimators: Nombre d'arbres.
  estimator__n_estimators: [100, 200]
  # estimator__max_depth: Profondeur maximale.
  estimator__max_depth: [3,5]

Hist Gradient Boosting Multi-label:
  estimator__learning_rate: [0.05, 0.1]
  estimator__max_iter: [100, 200]
  estimator__max_depth: [null, 10]
  estimator__min_samples_leaf: [20]

MLP Multi-label:
  hidden_layer_sizes: [[100], [100, 50]] # Un peu plus gros pour le multi-label
  activation: ['relu']
  alpha: [0.0001]
  max_iter: [1000]