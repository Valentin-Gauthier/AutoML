[load dataset] Loading Dense dataset: data_I
[fit] multiclass_classification task detected.

    Dataset Target Analysis (10000 samples)
 Type: Multiclass Classification [Balanced]
  -> Class 0  : 1988   samples (19.88%)
  -> Class 1  : 2049   samples (20.49%)
  -> Class 2  : 1913   samples (19.13%)
  -> Class 3  : 2046   samples (20.46%)
  -> Class 4  : 2004   samples (20.04%)


[fit] Dense features threshold exceeded (2000 > 500).
[fit] Reducing to the top 500 features...

[fit] Candidate models loaded for task 'multiclass_classification':
   1. Logistic Regression
   2. Hist Gradient Boosting
   3. K-Neighbors Classifier
   4. Gaussian Naive Bayes
   5. Bernoulli Naive Bayes
   6. SVC
   7. Random Forest Classifier
   8. Gradient Boosting Classifier

 [filter models] Dynamic Model Filtering ---
 Dataset Info: 8000 rows, 500 cols, Sparse=False
 -> Models selected: 8 / 8

[CLUSTER] Optimizing: Logistic Regression...
[CLUSTER] Success. Best params: {'C': 0.03744475735989316, 'solver': 'lbfgs', 'max_iter': 5000}
[fit] Retraining final model on full data...
[CLUSTER] Optimizing: Hist Gradient Boosting...
[CLUSTER] Success. Best params: {'learning_rate': 0.1419114328541414, 'max_iter': 448, 'max_depth': 10, 'l2_regularization': 0.06980143490221122}
[fit] Retraining final model on full data...
[CLUSTER] Optimizing: K-Neighbors Classifier...
[CLUSTER] Success. Best params: {'n_neighbors': 8, 'weights': 'uniform', 'p': 2, 'n_jobs': 1}
[fit] Retraining final model on full data...
[CLUSTER] Optimizing: Gaussian Naive Bayes...
[CLUSTER] Success. Best params: {'var_smoothing': 8.500538531854595e-06}
[fit] Retraining final model on full data...
[CLUSTER] Optimizing: Bernoulli Naive Bayes...
[CLUSTER] Success. Best params: {'alpha': 0.13645079556350523, 'binarize': 0.5999371943808456}
[fit] Retraining final model on full data...
[CLUSTER] Optimizing: SVC...
[CLUSTER] Success. Best params: {'C': 217.77553237707264, 'kernel': 'poly', 'gamma': 'scale', 'probability': True, 'max_iter': 10000}
[fit] Retraining final model on full data...
/info/etu/m1/s2501728/miniconda3/envs/autoML_env/lib/python3.12/site-packages/sklearn/svm/_base.py:305: ConvergenceWarning: Solver terminated early (max_iter=10000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  warnings.warn(
[CLUSTER] Optimizing: Random Forest Classifier...
[CLUSTER] Success. Best params: {'n_estimators': 396, 'max_depth': None, 'min_samples_split': 5, 'min_samples_leaf': 1, 'class_weight': 'balanced', 'n_jobs': 1}
[fit] Retraining final model on full data...
[CLUSTER] Optimizing: Gradient Boosting Classifier...
[CLUSTER] Success. Best params: {'n_estimators': 133, 'learning_rate': 0.01093404927607408, 'max_depth': 7, 'subsample': 0.8266166385241133}
[fit] Retraining final model on full data...

[eval] --- Detailed Results on Test Set (2000 samples) ---

 > Model: Logistic Regression
   - Accuracy : 0.8895
   - F1 Macro : 0.8894 (Balanced metric)

[Classification Report]
              precision    recall  f1-score   support

           0       0.90      0.86      0.88       428
           1       0.92      0.96      0.94       376
           2       0.86      0.84      0.85       371
           3       0.89      0.90      0.90       404
           4       0.87      0.88      0.88       421

    accuracy                           0.89      2000
   macro avg       0.89      0.89      0.89      2000
weighted avg       0.89      0.89      0.89      2000


 > Model: Hist Gradient Boosting
   - Accuracy : 0.9790
   - F1 Macro : 0.9791 (Balanced metric)

[Classification Report]
              precision    recall  f1-score   support

           0       0.99      0.98      0.99       428
           1       0.99      0.99      0.99       376
           2       0.98      0.98      0.98       371
           3       0.97      0.97      0.97       404
           4       0.97      0.97      0.97       421

    accuracy                           0.98      2000
   macro avg       0.98      0.98      0.98      2000
weighted avg       0.98      0.98      0.98      2000


 > Model: K-Neighbors Classifier
   - Accuracy : 0.9090
   - F1 Macro : 0.9095 (Balanced metric)

[Classification Report]
              precision    recall  f1-score   support

           0       0.91      0.91      0.91       428
           1       0.92      0.97      0.94       376
           2       0.95      0.87      0.91       371
           3       0.89      0.93      0.91       404
           4       0.88      0.87      0.88       421

    accuracy                           0.91      2000
   macro avg       0.91      0.91      0.91      2000
weighted avg       0.91      0.91      0.91      2000


 > Model: Gaussian Naive Bayes
   - Accuracy : 0.4775
   - F1 Macro : 0.4507 (Balanced metric)

[Classification Report]
              precision    recall  f1-score   support

           0       0.36      0.11      0.16       428
           1       0.51      0.88      0.64       376
           2       0.37      0.56      0.44       371
           3       0.61      0.48      0.54       404
           4       0.53      0.42      0.46       421

    accuracy                           0.48      2000
   macro avg       0.48      0.49      0.45      2000
weighted avg       0.48      0.48      0.44      2000


 > Model: Bernoulli Naive Bayes
   - Accuracy : 0.3985
   - F1 Macro : 0.3300 (Balanced metric)

[Classification Report]
              precision    recall  f1-score   support

           0       0.47      0.08      0.13       428
           1       0.38      0.90      0.53       376
           2       0.42      0.05      0.09       371
           3       0.42      0.52      0.46       404
           4       0.40      0.47      0.43       421

    accuracy                           0.40      2000
   macro avg       0.42      0.40      0.33      2000
weighted avg       0.42      0.40      0.33      2000


 > Model: SVC
   - Accuracy : 0.9605
   - F1 Macro : 0.9605 (Balanced metric)

[Classification Report]
              precision    recall  f1-score   support

           0       0.96      0.94      0.95       428
           1       0.97      0.98      0.97       376
           2       0.94      0.95      0.94       371
           3       0.96      0.97      0.96       404
           4       0.97      0.96      0.97       421

    accuracy                           0.96      2000
   macro avg       0.96      0.96      0.96      2000
weighted avg       0.96      0.96      0.96      2000


 > Model: Random Forest Classifier
   - Accuracy : 0.9380
   - F1 Macro : 0.9379 (Balanced metric)

[Classification Report]
              precision    recall  f1-score   support

           0       0.95      0.92      0.94       428
           1       0.94      0.97      0.96       376
           2       0.91      0.92      0.92       371
           3       0.96      0.93      0.94       404
           4       0.93      0.95      0.94       421

    accuracy                           0.94      2000
   macro avg       0.94      0.94      0.94      2000
weighted avg       0.94      0.94      0.94      2000


 > Model: Gradient Boosting Classifier
   - Accuracy : 0.9040
   - F1 Macro : 0.9044 (Balanced metric)

[Classification Report]
              precision    recall  f1-score   support

           0       0.95      0.87      0.91       428
           1       0.95      0.97      0.96       376
           2       0.86      0.88      0.87       371
           3       0.91      0.90      0.91       404
           4       0.86      0.91      0.88       421

    accuracy                           0.90      2000
   macro avg       0.91      0.90      0.90      2000
weighted avg       0.91      0.90      0.90      2000


==================================================
 BEST MODEL : Hist Gradient Boosting
 Score      : 0.9791
 Params     : {'learning_rate': 0.1419114328541414, 'max_iter': 448, 'max_depth': 10, 'l2_regularization': 0.06980143490221122}
==================================================