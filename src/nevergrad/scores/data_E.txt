[load dataset] Loading Dense dataset: data_E
[fit] binary_classification task detected.

    Dataset Target Analysis (3140 samples)
 Type: Binary Classification [Balanced]
  -> Class 0  : 1561   samples (49.71%)
  -> Class 1  : 1579   samples (50.29%)



[fit] Candidate models loaded for task 'binary_classification':
   1. Logistic Regression
   2. Hist Gradient Boosting
   3. K-Neighbors Classifier
   4. Gaussian Naive Bayes
   5. SVC
   6. Random Forest Classifier
   7. Bernoulli Naive Bayes
   8. Gradient Boosting Classifier
   9. MLP Classifier
   10. Linear SVC
   11. Extra Trees Classifier

 [filter models] Dynamic Model Filtering ---
 Dataset Info: 2512 rows, 259 cols, Sparse=False
 [EXCLUDED] Gaussian Naive Bayes............... : Incompatible with negative values (StandardScaler)
 [EXCLUDED] Bernoulli Naive Bayes.............. : Incompatible with negative values (StandardScaler)
 -> Models selected: 9 / 11

[CLUSTER] Optimizing: Logistic Regression...
[CLUSTER] Success (30.1s). Best params: {'C': 0.019019721470844624, 'solver': 'lbfgs', 'max_iter': 2000}
[fit] Retraining final model on full data...
[fit] Final training done in 2.9s.
[CLUSTER] Optimizing: Hist Gradient Boosting...
[CLUSTER] Success (113.9s). Best params: {'learning_rate': 0.01980356977647784, 'max_iter': 309, 'max_depth': 20, 'l2_regularization': 0.7866342746838411, 'early_stopping': True, 'n_iter_no_change': 10}
[fit] Retraining final model on full data...
[fit] Final training done in 3.5s.
[CLUSTER] Optimizing: K-Neighbors Classifier...
[CLUSTER] Success (40.8s). Best params: {'n_neighbors': 20, 'weights': 'distance', 'p': 1, 'n_jobs': 1}
[fit] Retraining final model on full data...
[fit] Final training done in 0.0s.
[CLUSTER] Optimizing: SVC...
[CLUSTER] Success (122.1s). Best params: {'C': 2.285942101288782, 'kernel': 'poly', 'gamma': 'scale', 'probability': True, 'max_iter': 2000}
[fit] Retraining final model on full data...
/info/etu/m1/s2501728/miniconda3/envs/autoML_env/lib/python3.12/site-packages/sklearn/svm/_base.py:305: ConvergenceWarning: Solver terminated early (max_iter=2000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  warnings.warn(
[fit] Final training done in 7.9s.
[CLUSTER] Optimizing: Random Forest Classifier...
[CLUSTER] Success (124.5s). Best params: {'n_estimators': 114, 'max_depth': 20, 'min_samples_split': 7, 'min_samples_leaf': 2, 'class_weight': 'balanced', 'n_jobs': 1}
[fit] Retraining final model on full data...
[fit] Final training done in 0.8s.
[CLUSTER] Optimizing: Gradient Boosting Classifier...
[CLUSTER] Success (258.7s). Best params: {'n_estimators': 130, 'learning_rate': 0.057391985364445855, 'max_depth': 6, 'subsample': 0.8355999918914865, 'n_iter_no_change': 10, 'validation_fraction': 0.1}
[fit] Retraining final model on full data...
[fit] Final training done in 41.6s.
[CLUSTER] Optimizing: MLP Classifier...
[CLUSTER] Success (50.1s). Best params: {'hidden_layer_sizes': [100, 50], 'activation': 'relu', 'alpha': 0.0008576453367129575, 'learning_rate_init': 0.006745466041775909, 'max_iter': 500, 'early_stopping': True, 'n_iter_no_change': 10}
[fit] Retraining final model on full data...
[fit] Final training done in 1.5s.
[CLUSTER] Optimizing: Linear SVC...
[CLUSTER] Success (30.7s). Best params: {'C': 0.01636384128868211, 'penalty': 'l2', 'max_iter': 2000}
[fit] Retraining final model on full data...
[fit] Final training done in 0.2s.
[CLUSTER] Optimizing: Extra Trees Classifier...
[CLUSTER] Success (62.7s). Best params: {'n_estimators': 181, 'max_depth': 20, 'min_samples_split': 7, 'n_jobs': 1}
[fit] Retraining final model on full data...
[fit] Final training done in 0.6s.

[eval] --- Detailed Results on Test Set (628 samples) ---

 > Model: Logistic Regression
   - Accuracy : 0.5876
   - F1 Macro : 0.5870 (Balanced metric)

[Classification Report]
              precision    recall  f1-score   support

         0.0       0.57      0.57      0.57       302
         1.0       0.60      0.60      0.60       326

    accuracy                           0.59       628
   macro avg       0.59      0.59      0.59       628
weighted avg       0.59      0.59      0.59       628


 > Model: Hist Gradient Boosting
   - Accuracy : 0.8360
   - F1 Macro : 0.8359 (Balanced metric)

[Classification Report]
              precision    recall  f1-score   support

         0.0       0.82      0.85      0.83       302
         1.0       0.86      0.82      0.84       326

    accuracy                           0.84       628
   macro avg       0.84      0.84      0.84       628
weighted avg       0.84      0.84      0.84       628


 > Model: K-Neighbors Classifier
   - Accuracy : 0.6704
   - F1 Macro : 0.6704 (Balanced metric)

[Classification Report]
              precision    recall  f1-score   support

         0.0       0.65      0.69      0.67       302
         1.0       0.70      0.65      0.67       326

    accuracy                           0.67       628
   macro avg       0.67      0.67      0.67       628
weighted avg       0.67      0.67      0.67       628


 > Model: SVC
   - Accuracy : 0.6146
   - F1 Macro : 0.6146 (Balanced metric)

[Classification Report]
              precision    recall  f1-score   support

         0.0       0.59      0.62      0.61       302
         1.0       0.63      0.61      0.62       326

    accuracy                           0.61       628
   macro avg       0.61      0.61      0.61       628
weighted avg       0.62      0.61      0.61       628


 > Model: Random Forest Classifier
   - Accuracy : 0.7866
   - F1 Macro : 0.7866 (Balanced metric)

[Classification Report]
              precision    recall  f1-score   support

         0.0       0.77      0.80      0.78       302
         1.0       0.81      0.77      0.79       326

    accuracy                           0.79       628
   macro avg       0.79      0.79      0.79       628
weighted avg       0.79      0.79      0.79       628


 > Model: Gradient Boosting Classifier
   - Accuracy : 0.8471
   - F1 Macro : 0.8470 (Balanced metric)

[Classification Report]
              precision    recall  f1-score   support

         0.0       0.84      0.85      0.84       302
         1.0       0.86      0.85      0.85       326

    accuracy                           0.85       628
   macro avg       0.85      0.85      0.85       628
weighted avg       0.85      0.85      0.85       628


 > Model: MLP Classifier
   - Accuracy : 0.5955
   - F1 Macro : 0.5939 (Balanced metric)

[Classification Report]
              precision    recall  f1-score   support

         0.0       0.58      0.55      0.57       302
         1.0       0.61      0.63      0.62       326

    accuracy                           0.60       628
   macro avg       0.59      0.59      0.59       628
weighted avg       0.59      0.60      0.59       628


 > Model: Linear SVC
   - Accuracy : 0.5812
   - F1 Macro : 0.5806 (Balanced metric)

[Classification Report]
              precision    recall  f1-score   support

         0.0       0.56      0.57      0.57       302
         1.0       0.60      0.60      0.60       326

    accuracy                           0.58       628
   macro avg       0.58      0.58      0.58       628
weighted avg       0.58      0.58      0.58       628


 > Model: Extra Trees Classifier
   - Accuracy : 0.7229
   - F1 Macro : 0.7229 (Balanced metric)

[Classification Report]
              precision    recall  f1-score   support

         0.0       0.70      0.75      0.72       302
         1.0       0.75      0.69      0.72       326

    accuracy                           0.72       628
   macro avg       0.72      0.72      0.72       628
weighted avg       0.73      0.72      0.72       628


==================================================
 BEST MODEL : Gradient Boosting Classifier
 Score      : 0.8470
 Params     : {'n_estimators': 130, 'learning_rate': 0.057391985364445855, 'max_depth': 6, 'subsample': 0.8355999918914865, 'n_iter_no_change': 10, 'validation_fraction': 0.1}
==================================================