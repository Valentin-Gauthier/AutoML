[load dataset] Loading Sparse dataset: data_H
[fit] multilabel_classification task detected.

    Dataset Target Analysis (45400 samples)
   Type: Multi-label (91 labels)
 - Average labels per sample: 1.43
 -> Label 72 : 5353   (11.79%)
 -> Label 52 : 5183   (11.42%)
 -> Label 23 : 2862   (6.30%)
 -> Label 76 : 2803   (6.17%)
 -> Label 6  : 2338   (5.15%)
 -> Label 1  : 2223   (4.90%)
 -> Label 65 : 2199   (4.84%)
 -> Label 70 : 2134   (4.70%)
 -> Label 43 : 2068   (4.56%)
 -> Label 86 : 1741   (3.83%)
 -> Label 36 : 1703   (3.75%)
 -> Label 46 : 1479   (3.26%)
 -> Label 20 : 1438   (3.17%)
 -> Label 22 : 1363   (3.00%)
 -> Label 62 : 1289   (2.84%)
 -> Label 54 : 1163   (2.56%)
 -> Label 48 : 1133   (2.50%)
 -> Label 67 : 1102   (2.43%)
 -> Label 3  : 1082   (2.38%)
 -> Label 30 : 1041   (2.29%)
 -> Label 33 : 977    (2.15%)
 -> Label 59 : 847    (1.87%)
 -> Label 64 : 845    (1.86%)
 -> Label 18 : 807    (1.78%)
 -> Label 8  : 778    (1.71%)
 -> Label 19 : 757    (1.67%)
 -> Label 63 : 636    (1.40%)
 -> Label 24 : 618    (1.36%)
 -> Label 60 : 617    (1.36%)
 -> Label 39 : 612    (1.35%)
 -> Label 7  : 611    (1.35%)
 -> Label 29 : 608    (1.34%)
 -> Label 42 : 603    (1.33%)
 -> Label 27 : 554    (1.22%)
 -> Label 11 : 548    (1.21%)
 -> Label 75 : 521    (1.15%)
 -> Label 71 : 521    (1.15%)
 -> Label 49 : 514    (1.13%)
 -> Label 80 : 502    (1.11%)
 -> Label 21 : 500    (1.10%)
 -> Label 17 : 464    (1.02%)
 -> Label 84 : 452    (1.00%)
 -> Label 9  : 434    (0.96%)
 -> Label 50 : 422    (0.93%)
 -> Label 73 : 398    (0.88%)
 -> Label 57 : 378    (0.83%)
 -> Label 45 : 359    (0.79%)
 -> Label 16 : 317    (0.70%)
 -> Label 68 : 317    (0.70%)
 -> Label 66 : 302    (0.67%)
 -> Label 26 : 287    (0.63%)
 -> Label 88 : 250    (0.55%)
 -> Label 2  : 250    (0.55%)
 -> Label 5  : 246    (0.54%)
 -> Label 89 : 246    (0.54%)
 -> Label 0  : 242    (0.53%)
 -> Label 4  : 237    (0.52%)
 -> Label 58 : 225    (0.50%)
 -> Label 79 : 217    (0.48%)
 -> Label 55 : 204    (0.45%)
 -> Label 38 : 203    (0.45%)
 -> Label 37 : 202    (0.44%)
 -> Label 61 : 201    (0.44%)
 -> Label 44 : 200    (0.44%)
 -> Label 74 : 195    (0.43%)
 -> Label 28 : 190    (0.42%)
 -> Label 47 : 178    (0.39%)
 -> Label 82 : 164    (0.36%)
 -> Label 87 : 161    (0.35%)
 -> Label 10 : 161    (0.35%)
 -> Label 78 : 157    (0.35%)
 -> Label 34 : 156    (0.34%)
 -> Label 15 : 146    (0.32%)
 -> Label 51 : 129    (0.28%)
 -> Label 14 : 127    (0.28%)
 -> Label 83 : 125    (0.28%)
 -> Label 77 : 124    (0.27%)
 -> Label 13 : 118    (0.26%)
 -> Label 90 : 118    (0.26%)
 -> Label 31 : 112    (0.25%)
 -> Label 85 : 109    (0.24%)
 -> Label 32 : 107    (0.24%)
 -> Label 56 : 104    (0.23%)
 -> Label 40 : 104    (0.23%)
 -> Label 53 : 99     (0.22%)
 -> Label 25 : 87     (0.19%)
 -> Label 12 : 85     (0.19%)
 -> Label 41 : 82     (0.18%)
 -> Label 69 : 73     (0.16%)
 -> Label 35 : 73     (0.16%)
 -> Label 81 : 49     (0.11%)


[fit] Features threshold exceeded (301562 > 500).
[fit] Reducing to the top 500 features...
[fit] Reduction done. New shape: (36320, 500)

[fit] Candidate models loaded for task 'multilabel_classification':
   1. Random Forest Classifier Multi-label
   2. K-Neighbors Classifier Multi-label
   3. Gradient Boosting Classifier Multi-label
   4. Hist Gradient Boosting Multi-label
   5. MLP Multi-label
   6. Ridge Classifier

 [filter models] Dynamic Model Filtering ---
 Dataset Info: 36320 rows, 500 cols, Sparse=True
 Target Info : 91 labels (Multi-label)
 [EXCLUDED] K-Neighbors Classifier Multi-label. : Too heavy for 91 labels (requires 91 models)
 [EXCLUDED] Gradient Boosting Classifier Multi-label : Too heavy for 91 labels (requires 91 models)
 [EXCLUDED] Hist Gradient Boosting Multi-label. : Incompatible with Sparse data
 -> Models selected: 3 / 6

[CLUSTER] Optimizing: Random Forest Classifier Multi-label...
[CLUSTER] Success. Best params: {'estimator__n_estimators': 220, 'estimator__max_depth': None, 'estimator__min_samples_leaf': 3, 'estimator__n_jobs': 1}
[fit] Retraining final model on full data...
[CLUSTER] Optimizing: MLP Multi-label...
[CLUSTER] Success. Best params: {'hidden_layer_sizes': [100], 'activation': 'relu', 'alpha': 0.0011552430177881724, 'learning_rate_init': 0.003846098675643806, 'max_iter': 1000, 'early_stopping': True, 'n_iter_no_change': 10}
[fit] Retraining final model on full data...
[CLUSTER] Optimizing: Ridge Classifier...
[CLUSTER] Success. Best params: {'estimator__alpha': 0.1573124217026009}
[fit] Retraining final model on full data...

[eval] --- Detailed Results on Test Set (9080 samples) ---

 > Model: Random Forest Classifier Multi-label
   - F1 (Samples): 0.4834 (Quality per instance)
   - F1 (Macro)  : 0.4544 (Quality per label)

[Classification Report]
              precision    recall  f1-score   support

           0       0.00      0.00      0.00        58
           1       0.70      0.45      0.55       456
           2       0.00      0.00      0.00        61
           3       0.95      0.65      0.77       220
           4       0.86      0.45      0.59        42
           5       0.00      0.00      0.00        47
           6       0.92      0.89      0.90       451
           7       0.00      0.00      0.00       115
           8       0.88      0.68      0.76       160
           9       0.00      0.00      0.00        89
          10       1.00      0.65      0.79        40
          11       0.74      0.20      0.31        87
          12       1.00      0.31      0.47        13
          13       1.00      0.04      0.07        27
          14       0.73      0.38      0.50        21
          15       1.00      0.37      0.54        27
          16       1.00      0.31      0.48        54
          17       0.00      0.00      0.00        94
          18       0.89      0.81      0.85       166
          19       0.88      0.34      0.49       128
          20       0.86      0.79      0.82       256
          21       0.90      0.56      0.69        99
          22       0.85      0.71      0.77       292
          23       0.00      0.00      0.00       598
          24       0.88      0.71      0.79       121
          25       1.00      0.13      0.24        15
          26       1.00      0.31      0.47        62
          27       0.95      0.63      0.76       113
          28       0.00      0.00      0.00        41
          29       0.96      0.19      0.32       120
          30       0.94      0.14      0.24       224
          31       0.86      0.29      0.43        21
          32       0.78      0.32      0.45        22
          33       0.80      0.35      0.49       195
          34       1.00      0.69      0.82        39
          35       0.00      0.00      0.00        14
          36       0.79      0.47      0.59       347
          37       0.81      0.52      0.63        33
          38       0.94      0.60      0.73        50
          39       0.92      0.44      0.59       124
          40       0.90      0.33      0.49        27
          41       1.00      0.44      0.61        16
          42       0.97      0.61      0.75       118
          43       0.81      0.55      0.65       422
          44       0.00      0.00      0.00        50
          45       0.98      0.73      0.84        79
          46       0.95      0.42      0.58       293
          47       0.75      0.41      0.53        22
          48       0.90      0.39      0.55       222
          49       0.71      0.11      0.20        88
          50       0.00      0.00      0.00        81
          51       0.50      0.05      0.08        22
          52       0.89      0.85      0.87      1032
          53       1.00      0.12      0.21        17
          54       0.78      0.28      0.41       237
          55       1.00      0.07      0.13        43
          56       1.00      0.41      0.58        17
          57       0.93      0.68      0.79        84
          58       0.00      0.00      0.00        49
          59       0.99      0.37      0.54       179
          60       0.00      0.00      0.00       116
          61       1.00      0.23      0.38        39
          62       0.93      0.44      0.60       283
          63       0.91      0.70      0.79       115
          64       0.91      0.51      0.65       175
          65       0.73      0.04      0.07       421
          66       0.91      0.70      0.79        56
          67       1.00      0.06      0.11       227
          68       0.89      0.31      0.46        52
          69       1.00      0.69      0.81        16
          70       0.84      0.43      0.56       425
          71       0.00      0.00      0.00       105
          72       0.82      0.31      0.45      1081
          73       0.93      0.57      0.71        75
          74       1.00      0.36      0.52        45
          75       0.93      0.34      0.50       118
          76       0.80      0.62      0.70       563
          77       0.20      0.04      0.06        26
          78       1.00      0.76      0.87        34
          79       0.00      0.00      0.00        50
          80       0.96      0.29      0.45        93
          81       0.00      0.00      0.00         8
          82       0.84      0.62      0.71        34
          83       1.00      0.19      0.32        16
          84       0.00      0.00      0.00        96
          85       0.00      0.00      0.00        24
          86       0.94      0.81      0.87       328
          87       0.97      0.80      0.88        35
          88       0.97      0.79      0.87        43
          89       0.85      0.67      0.75        51
          90       0.94      0.67      0.78        24

   micro avg       0.87      0.44      0.58     13014
   macro avg       0.72      0.36      0.45     13014
weighted avg       0.75      0.44      0.53     13014
 samples avg       0.53      0.47      0.48     13014


 > Model: MLP Multi-label
   - F1 (Samples): 0.5307 (Quality per instance)
   - F1 (Macro)  : 0.5481 (Quality per label)

[Classification Report]
              precision    recall  f1-score   support

           0       0.33      0.02      0.03        58
           1       0.74      0.43      0.55       456
           2       0.50      0.02      0.03        61
           3       0.94      0.68      0.79       220
           4       0.83      0.71      0.77        42
           5       0.47      0.15      0.23        47
           6       0.94      0.87      0.90       451
           7       0.00      0.00      0.00       115
           8       0.90      0.71      0.79       160
           9       0.20      0.01      0.02        89
          10       1.00      0.78      0.87        40
          11       0.69      0.36      0.47        87
          12       1.00      0.46      0.63        13
          13       0.55      0.59      0.57        27
          14       0.77      0.48      0.59        21
          15       0.87      0.48      0.62        27
          16       0.88      0.56      0.68        54
          17       1.00      0.01      0.02        94
          18       0.90      0.82      0.86       166
          19       0.84      0.38      0.53       128
          20       0.85      0.84      0.84       256
          21       0.83      0.64      0.72        99
          22       0.82      0.79      0.81       292
          23       0.50      0.05      0.09       598
          24       0.90      0.72      0.80       121
          25       0.64      0.47      0.54        15
          26       0.74      0.74      0.74        62
          27       0.94      0.60      0.74       113
          28       0.00      0.00      0.00        41
          29       0.62      0.35      0.45       120
          30       0.74      0.38      0.50       224
          31       0.69      0.43      0.53        21
          32       0.89      0.36      0.52        22
          33       0.74      0.47      0.57       195
          34       1.00      0.72      0.84        39
          35       0.00      0.00      0.00        14
          36       0.74      0.52      0.61       347
          37       0.84      0.64      0.72        33
          38       0.93      0.82      0.87        50
          39       0.93      0.44      0.59       124
          40       0.79      0.41      0.54        27
          41       1.00      0.50      0.67        16
          42       0.97      0.58      0.73       118
          43       0.79      0.60      0.68       422
          44       0.00      0.00      0.00        50
          45       1.00      0.70      0.82        79
          46       0.89      0.46      0.60       293
          47       0.81      0.59      0.68        22
          48       0.86      0.44      0.58       222
          49       0.63      0.31      0.41        88
          50       1.00      0.01      0.02        81
          51       0.50      0.05      0.08        22
          52       0.90      0.84      0.87      1032
          53       1.00      0.47      0.64        17
          54       0.68      0.32      0.43       237
          55       1.00      0.47      0.63        43
          56       0.90      0.53      0.67        17
          57       0.98      0.73      0.84        84
          58       0.60      0.18      0.28        49
          59       0.98      0.36      0.53       179
          60       0.17      0.01      0.02       116
          61       0.71      0.44      0.54        39
          62       0.88      0.46      0.60       283
          63       0.88      0.74      0.80       115
          64       0.81      0.69      0.74       175
          65       0.53      0.15      0.23       421
          66       0.87      0.84      0.85        56
          67       1.00      0.07      0.12       227
          68       0.74      0.54      0.62        52
          69       0.92      0.75      0.83        16
          70       0.75      0.54      0.63       425
          71       0.74      0.32      0.45       105
          72       0.73      0.38      0.50      1081
          73       0.87      0.63      0.73        75
          74       0.81      0.58      0.68        45
          75       0.88      0.58      0.70       118
          76       0.79      0.62      0.69       563
          77       0.38      0.12      0.18        26
          78       1.00      0.88      0.94        34
          79       0.00      0.00      0.00        50
          80       0.70      0.55      0.61        93
          81       1.00      0.25      0.40         8
          82       0.78      0.74      0.76        34
          83       0.62      0.31      0.42        16
          84       0.57      0.14      0.22        96
          85       0.67      0.17      0.27        24
          86       0.94      0.78      0.86       328
          87       0.97      0.83      0.89        35
          88       1.00      0.79      0.88        43
          89       0.85      0.67      0.75        51
          90       0.80      0.83      0.82        24

   micro avg       0.83      0.50      0.62     13014
   macro avg       0.75      0.47      0.55     13014
weighted avg       0.77      0.50      0.58     13014
 samples avg       0.57      0.52      0.53     13014


 > Model: Ridge Classifier
   - F1 (Samples): 0.3679 (Quality per instance)
   - F1 (Macro)  : 0.4249 (Quality per label)

[Classification Report]
              precision    recall  f1-score   support

           0       0.00      0.00      0.00        58
           1       0.80      0.34      0.48       456
           2       1.00      0.02      0.03        61
           3       0.98      0.54      0.69       220
           4       0.89      0.38      0.53        42
           5       0.00      0.00      0.00        47
           6       0.94      0.75      0.84       451
           7       0.00      0.00      0.00       115
           8       0.82      0.44      0.57       160
           9       0.00      0.00      0.00        89
          10       1.00      0.53      0.69        40
          11       0.80      0.28      0.41        87
          12       1.00      0.23      0.38        13
          13       1.00      0.11      0.20        27
          14       0.65      0.52      0.58        21
          15       1.00      0.41      0.58        27
          16       0.90      0.33      0.49        54
          17       0.00      0.00      0.00        94
          18       0.95      0.53      0.68       166
          19       0.81      0.27      0.41       128
          20       0.81      0.70      0.75       256
          21       0.78      0.40      0.53        99
          22       0.88      0.55      0.68       292
          23       0.00      0.00      0.00       598
          24       0.93      0.44      0.60       121
          25       0.56      0.33      0.42        15
          26       0.96      0.44      0.60        62
          27       0.93      0.44      0.60       113
          28       0.00      0.00      0.00        41
          29       0.86      0.15      0.26       120
          30       0.74      0.25      0.37       224
          31       0.73      0.52      0.61        21
          32       0.89      0.36      0.52        22
          33       0.79      0.30      0.44       195
          34       1.00      0.56      0.72        39
          35       0.00      0.00      0.00        14
          36       0.84      0.41      0.55       347
          37       1.00      0.33      0.50        33
          38       0.97      0.56      0.71        50
          39       0.97      0.31      0.48       124
          40       0.75      0.33      0.46        27
          41       1.00      0.44      0.61        16
          42       1.00      0.29      0.45       118
          43       0.84      0.37      0.52       422
          44       0.00      0.00      0.00        50
          45       1.00      0.30      0.47        79
          46       0.89      0.27      0.41       293
          47       0.73      0.36      0.48        22
          48       0.92      0.32      0.47       222
          49       0.77      0.23      0.35        88
          50       0.00      0.00      0.00        81
          51       0.50      0.05      0.08        22
          52       0.93      0.58      0.72      1032
          53       0.86      0.35      0.50        17
          54       0.84      0.22      0.34       237
          55       1.00      0.37      0.54        43
          56       0.89      0.47      0.62        17
          57       0.97      0.67      0.79        84
          58       0.00      0.00      0.00        49
          59       1.00      0.23      0.37       179
          60       0.00      0.00      0.00       116
          61       0.76      0.33      0.46        39
          62       0.96      0.29      0.45       283
          63       0.91      0.58      0.71       115
          64       0.82      0.48      0.61       175
          65       0.31      0.02      0.04       421
          66       0.91      0.54      0.67        56
          67       0.92      0.05      0.09       227
          68       0.74      0.38      0.51        52
          69       1.00      0.69      0.81        16
          70       0.80      0.36      0.50       425
          71       0.37      0.07      0.11       105
          72       0.80      0.22      0.35      1081
          73       0.91      0.55      0.68        75
          74       0.90      0.42      0.58        45
          75       0.82      0.36      0.50       118
          76       0.82      0.36      0.50       563
          77       0.50      0.04      0.07        26
          78       1.00      0.56      0.72        34
          79       0.00      0.00      0.00        50
          80       0.73      0.24      0.36        93
          81       0.50      0.12      0.20         8
          82       0.83      0.56      0.67        34
          83       0.75      0.19      0.30        16
          84       0.00      0.00      0.00        96
          85       1.00      0.12      0.22        24
          86       0.92      0.55      0.69       328
          87       0.95      0.57      0.71        35
          88       1.00      0.63      0.77        43
          89       0.91      0.39      0.55        51
          90       0.89      0.71      0.79        24

   micro avg       0.87      0.34      0.48     13014
   macro avg       0.72      0.32      0.42     13014
weighted avg       0.75      0.34      0.45     13014
 samples avg       0.40      0.36      0.37     13014


==================================================
 BEST MODEL : MLP Multi-label
 Score      : 0.5307
 Params     : {'hidden_layer_sizes': [100], 'activation': 'relu', 'alpha': 0.0011552430177881724, 'learning_rate_init': 0.003846098675643806, 'max_iter': 1000, 'early_stopping': True, 'n_iter_no_change': 10}
==================================================